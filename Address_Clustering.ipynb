{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqIbj2McMzBN"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cTCNe6ugpkr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "pd.set_option('display.max_columns', None)\n",
        "import timeit\n",
        "pd.set_option('display.max_rows',None)\n",
        "import re\n",
        "import datetime\n",
        "import time\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from functools import wraps\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQwSEipBM4hy"
      },
      "source": [
        "# Execution time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9bEUNgCeCA1t"
      },
      "outputs": [],
      "source": [
        "#takes a single argument for the function to be decorated\n",
        "def timeit(func):\n",
        "    \"\"\"\n",
        "    Decorator function to measure the execution time of a wrapped function.\n",
        "\n",
        "    This decorator wraps the given function and measures the time taken by the function to execute using the\n",
        "    `time.perf_counter()` function from the Python `time` module. It prints the execution time in seconds and returns\n",
        "    the result of the wrapped function.\n",
        "\n",
        "    Args:\n",
        "        func (callable): The function to be wrapped and timed.\n",
        "\n",
        "    Returns:\n",
        "        callable: The wrapped function.\n",
        "\n",
        "    Example:\n",
        "        @timeit\n",
        "        def my_function():\n",
        "            # Code to be timed\n",
        "            # ...\n",
        "\n",
        "        my_function()  # This will print the execution time of my_function\n",
        "    \"\"\"\n",
        "    #its used to preserve the original functions metadata\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        #time.perf_counter() function from the Python time module to measure the time taken by the wrapped function to execute.\n",
        "        start = time.perf_counter()\n",
        "        result = func(*args, **kwargs)\n",
        "        #end time\n",
        "        end = time.perf_counter()\n",
        "        #final\n",
        "        f=open(\"output.txt\",\"a\")\n",
        "        print(f'{func.__name__} took {end - start:.6f} seconds to complete',file=f)\n",
        "        f.close()\n",
        "        return result\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "8QSV8_bag7zl",
        "outputId": "ee787148-6758-407d-aad5-b686c07a25dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Overall, converting the data to tensors and performing preprocessing operations\\n can help reduce memory occupancy and enable more efficient memory management when working with large datasets.'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "'''Overall, converting the data to tensors and performing preprocessing operations\n",
        " can help reduce memory occupancy and enable more efficient memory management when working with large datasets.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3SVghH5NF1k"
      },
      "source": [
        "# Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "otuWPdR6ytRn"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "  \"\"\"\n",
        "  Create a TensorFlow Dataset from a Pandas DataFrame.\n",
        "\n",
        "  This function takes a Pandas DataFrame and converts it into a TensorFlow Dataset using the `from_tensor_slices`\n",
        "  method. The DataFrame is converted into a dictionary, where the keys represent column names and the values represent\n",
        "  the corresponding data arrays or Series.\n",
        "\n",
        "  Args:\n",
        "      df (pandas.DataFrame): The input DataFrame to convert into a TensorFlow Dataset.\n",
        "\n",
        "  Returns:\n",
        "      tf.data.Dataset: The TensorFlow Dataset created from the input DataFrame.\n",
        "\n",
        "  Example:\n",
        "      import pandas as pd\n",
        "      import tensorflow as tf\n",
        "\n",
        "      # Create a Pandas DataFrame\n",
        "      df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\n",
        "\n",
        "      # Create TensorFlow Dataset from DataFrame\n",
        "      dataset = create_dataset_from_dataframe(df)\n",
        "  \"\"\"\n",
        "  df=pd.read_csv(\"Synthetic_Data_Address_clustering_sample (1).csv\")\n",
        "\n",
        "  df_tf=tf.data.Dataset.from_tensor_slices(dict(df))\n",
        "\n",
        "  #tensorflow datasets are good for large scale data, shuffling, batching and pre-fetching\n",
        "  return df_tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_tf=load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtJ2FJKCNLfk"
      },
      "source": [
        "# Applying Standardization(lowercase, extra spaces, null vales, duplicates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ascii_sum(dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to calculate the ASCII sum of the 'full_address' column in a TensorFlow dataset.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (tf.data.Dataset): Input TensorFlow dataset.\n",
        "\n",
        "    Return:\n",
        "        tf.data.Dataset: A new TensorFlow dataset with the 'ascii_sum' column added.\n",
        "    \"\"\"\n",
        "    row = dataset.copy()\n",
        "\n",
        "    # Convert the 'full_address' string to Unicode code points\n",
        "    unicode_code_points = tf.strings.unicode_decode(row['Delivery_Desc'], 'UTF-8')\n",
        "\n",
        "    # Calculate the sum of Unicode code points to get the ASCII sum\n",
        "    ascii_sum_value = tf.reduce_sum(unicode_code_points)\n",
        "\n",
        "    # Update the row dictionary with the 'ascii_sum' column\n",
        "    row['ascii_sum'] = ascii_sum_value\n",
        "\n",
        "    return row   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "@timeit\n",
        "def remove_duplicates(s):\n",
        "    words = tf.strings.reduce_join(s, separator=', ')\n",
        "    words = tf.strings.split(words, sep=', ')\n",
        "    _, idx = tf.unique(words)\n",
        "    unique_words = tf.gather(words, idx)\n",
        "    return tf.strings.reduce_join(unique_words, separator=', ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gO8rI5Rj1DeK"
      },
      "outputs": [],
      "source": [
        "#execution time for this function will be computed\n",
        "\n",
        "\n",
        "#to enable graph mode(eager tensor is by default there)-----reduce_retracing, experimental_relax_shapes----experimental purposes\n",
        "# @tf.function(reduce_retracing=True,experimental_relax_shapes=False)\n",
        "\n",
        "#this function takes inputs as each element(row) in a tensor and returns the processed result\n",
        "@timeit\n",
        "def standardization(element):\n",
        "    \"\"\"\n",
        "      Standardize the 'Delivery_Desc' column of the input element.\n",
        "\n",
        "      This function performs standardization operations on the 'Delivery_Desc' column of the input element using TensorFlow\n",
        "      operations. The operations include converting the text to lowercase, removing special characters, removing the word\n",
        "      'null' (if present), removing extra white spaces, removing duplicates, and rejoining the words into a single string.\n",
        "\n",
        "      Args:\n",
        "          element (dict): The input element containing the 'Delivery_Desc' column.\n",
        "\n",
        "      Returns:\n",
        "          dict: The standardized element with the 'Delivery_Desc' column processed.\n",
        "\n",
        "      Example:\n",
        "          import tensorflow as tf\n",
        "\n",
        "          # Define an input element\n",
        "          element = {'Delivery_Desc': 'This is a Sample Description.'}\n",
        "\n",
        "          # Standardize the input element\n",
        "          standardized_element = standardization(element)\n",
        "    \"\"\"\n",
        "    element_copy = element.copy()\n",
        "    #converting to lower case for each element\n",
        "    element_copy['Delivery_Desc'] = tf.strings.lower(element_copy['Delivery_Desc'])\n",
        "\n",
        "    #removing all special characters(not from a-z or 0-9)\n",
        "    element_copy['Delivery_Desc']=tf.strings.regex_replace(element_copy['Delivery_Desc'], r\"[^a-zA-Z0-9]\", \" \")\n",
        "\n",
        "    #removing words 'null' or 'none'(but no 'none' word was found in the dataset)---\\b represents boundary\n",
        "    element_copy['Delivery_Desc']=tf.strings.regex_replace(element_copy['Delivery_Desc'],r\"\\bnull\\b\",\" \")\n",
        "\n",
        "    #removing extra white spaces( \\s+ for a space after a white space)\n",
        "    element_copy['Delivery_Desc']=tf.strings.regex_replace(element_copy['Delivery_Desc'],r\"\\s+\",\" \")\n",
        "\n",
        "    #stripping white spaces before and after the string\n",
        "    element_copy['Delivery_Desc'] = tf.strings.strip(element_copy['Delivery_Desc'])\n",
        "\n",
        "    #removing duplicates--->splitting the words individually converts it to a 'ragged tensor'(variable length)\n",
        "    element_copy['Delivery_Desc'] = tf.strings.split(element_copy['Delivery_Desc'], sep=' ')\n",
        "\n",
        "    element_copy['Delivery_Desc'] = remove_duplicates(element_copy['Delivery_Desc'])\n",
        "    element_copy['Delivery_Desc'] = tf.strings.reduce_join(tf.unique(tf.strings.split(element_copy['Delivery_Desc'], sep=' '))[0], axis=-1, separator=' ')\n",
        "\n",
        "    return element_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SHhpmtmuiQ3R"
      },
      "outputs": [],
      "source": [
        "#eliminate short words of less than 20 characters\n",
        "@timeit\n",
        "def eliminate_short_words(element):\n",
        "    \"\"\"\n",
        "    Eliminate short words from the 'Delivery_Desc' column of the input element.\n",
        "\n",
        "    This function removes short words from the 'Delivery_Desc' column of the input element. It checks the length of each\n",
        "    word in the 'Delivery_Desc' column and retains only the words that have a length greater than or equal to 20\n",
        "    characters.\n",
        "\n",
        "    Args:\n",
        "        element (dict): The input element containing the 'Delivery_Desc' column.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: A boolean tensor representing the words with length greater than or equal to 20 characters.\n",
        "\n",
        "    Example:\n",
        "        import tensorflow as tf\n",
        "\n",
        "        # Define an input element\n",
        "        element = {'Delivery_Desc': 'This is a Sample Description with some long words.'}\n",
        "\n",
        "        # Eliminate short words from the input element\n",
        "        long_words = eliminate_short_words(element)\n",
        "        print(long_words)\n",
        "        # Output: [False, False, False, False, False, True, True, True, True]\n",
        "    \"\"\"\n",
        "    #passing in elements greater than 20 characters which later is retained\n",
        "    words=tf.strings.length(element['Delivery_Desc'])>=20\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Jsf9r3G-AueE"
      },
      "outputs": [],
      "source": [
        "bow=pd.read_csv(\"Bag_of_words.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMfAbPFzMpfm"
      },
      "source": [
        "\n",
        "# Standardization with bag of words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3o6BjK-wNPxz"
      },
      "outputs": [],
      "source": [
        "# Create combinations for regex\n",
        "#?< means check in negative direction,,, ![\\w\\d] check if something is not a word or a digit\n",
        "# Create combinations for regex\n",
        "starting = r\"\\b(?:\"\n",
        "middle = r\"|\"\n",
        "ending = r\")\\b\"\n",
        "\n",
        "#storing all the similar words in corresponding variables in the form of strings\n",
        "house_number=np.array(bow['word2'][1:40])\n",
        "ward_number=np.array(bow['word4'][1:25])\n",
        "flat_no=np.array(bow['word6'][1:8])\n",
        "plot_no=np.array(bow['word8'][1:7])\n",
        "room_no=np.array(bow['word10'][1:19])\n",
        "door_no=np.array(bow['word12'][1:12])\n",
        "quarter_no=np.array(bow['word14'][1:18])\n",
        "apartment_no=np.array(bow['word16'][1:20])\n",
        "duplex_no=np.array(bow['word18'][1:14])\n",
        "coop_list=np.array(bow['word20'][1:18])\n",
        "society_list=np.array(bow['word22'][1:24])\n",
        "road=np.array(bow['word24'][1:31])\n",
        "street_no=np.array(bow['word26'][1:21])\n",
        "nagar=np.array(bow['word28'][1:15])\n",
        "behind=np.array(bow['word30'][1:26])\n",
        "before=np.array(bow['word32'][1:17])\n",
        "near=np.array(bow['word34'][1:36])\n",
        "next_to=np.array(bow['word36'][1:26])\n",
        "opposite=np.array(bow['word38'][1:31])\n",
        "front=np.array(bow['word40'][1:21])\n",
        "after=np.array(bow['word42'][1:18])\n",
        "\n",
        "# Creating regex string for all combinations of stop words, checking for boundaries(spaces before the word or after the word)\n",
        "\n",
        "house_number = starting + middle.join(house_number) + ending\n",
        "ward_number = starting + middle.join(ward_number) + ending\n",
        "flat_no = starting + middle.join(flat_no) + ending\n",
        "plot_no = starting + middle.join(plot_no) + ending\n",
        "room_no = starting + middle.join(room_no) + ending\n",
        "door_no = starting + middle.join(door_no) + ending\n",
        "quarter_no = starting + middle.join(quarter_no) + ending\n",
        "apartment_no = starting + middle.join(apartment_no) + ending\n",
        "duplex_no = starting + middle.join(duplex_no) + ending\n",
        "coop_list = starting + middle.join(coop_list) + ending\n",
        "society_list = starting + middle.join(society_list) + ending\n",
        "road= starting + middle.join(road) + ending\n",
        "street_no= starting + middle.join(street_no) + ending\n",
        "nagar = starting + middle.join(nagar) + ending\n",
        "behind = starting + middle.join(behind) + ending\n",
        "before = starting + middle.join(before) + ending\n",
        "near = starting + middle.join(near) + ending\n",
        "next_to = starting + middle.join(next_to) + ending\n",
        "opposite = starting + middle.join(opposite) + ending\n",
        "front = starting + middle.join(front) + ending\n",
        "after = starting + middle.join(after) + ending\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ItGuCHoB1usr"
      },
      "outputs": [],
      "source": [
        "#Calculating the time for this function\n",
        "\n",
        "#this function replaces all the similar words with bag of words\n",
        "@timeit\n",
        "def replace_strings(tensor_dataset):\n",
        "    row = tensor_dataset.copy()\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], house_number, ' house ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], ward_number, ' ward ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], flat_no, ' flat ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], plot_no, ' plot ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], room_no, ' room ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], door_no, ' door ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], quarter_no, ' quarter ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], apartment_no, ' apartment ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], duplex_no, ' duplex ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], coop_list, ' coop ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], society_list, ' society ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], road, ' road ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], street_no, ' street ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], nagar, ' nagar ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], behind, ' behind ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], before, ' before ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], near, ' near ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], next_to, ' next ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], opposite, ' opposite ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], front, ' front ')\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], after, ' after ')\n",
        "\n",
        "    return row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to remove words like - house, flat, room, etc\n",
        "@timeit\n",
        "def remove_stop_words(element):\n",
        "    row=element.copy()\n",
        "    row['Delivery_Desc'] = tf.strings.regex_replace(row['Delivery_Desc'], r\"\\b(?:house|ward|flat|plot|room|door|quarter|apartment|duplex|coop|society|road|street|nagar|behind|before|near|next|opposite|front|after)\\b\", \"\")\n",
        "    return row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2xa7Po5MRco"
      },
      "source": [
        "# Main Function to call all Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9ZCC8W2rJ91Q"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "      # creating tensorflow dataset from csv\n",
        "      df_tf = load_data()\n",
        "\n",
        "      df_tf = df_tf.map(standardization)\n",
        "\n",
        "      #Removing rows with only numbers\n",
        "      df_tf = df_tf.filter(lambda x: tf.math.logical_not(tf.strings.regex_full_match(x['Delivery_Desc'], r'^[0-9]+$')))\n",
        "\n",
        "      #filtering the dataset by keeping the retained words\n",
        "      df_tf=df_tf.filter(eliminate_short_words)\n",
        "\n",
        "      #implementing bag of wors standardization\n",
        "      df_tf = df_tf.map(replace_strings)\n",
        "      df_tf = df_tf.map(remove_stop_words)\n",
        "\n",
        "      return df_tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_gdtWetQGB-"
      },
      "source": [
        "# Calling Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QiGEoUJP5uOy"
      },
      "outputs": [],
      "source": [
        "df_tf = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in df_tf:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting it back to a pandas dataframe due to drawbacks(please navigate to bge.ipynb for further results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "#converting tf dataset to pandas dataframe\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(df_tf.as_numpy_iterator())\n",
        "df.to_csv('output.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hypothetically if the sorting was possible in tensorflow dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity(x, y):\n",
        "    # Ensure length of x and y are the same\n",
        "    if len(x) != len(y) :\n",
        "        return None\n",
        "    \n",
        "    # Compute the dot product between x and y\n",
        "    dot_product = np.dot(x, y)\n",
        "    \n",
        "    # Compute the L2 norms (magnitudes) of x and y\n",
        "    magnitude_x = np.sqrt(np.sum(x**2))\n",
        "    magnitude_y = np.sqrt(np.sum(y**2))\n",
        "    \n",
        "    # Compute the cosine similarity\n",
        "    cosine_similarity = dot_product / (magnitude_x * magnitude_y)\n",
        "    \n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def euclidean_distance(vector1, vector2):\n",
        "    if len(vector1) != len(vector2):\n",
        "        raise ValueError(\"Vectors must have the same dimension\")\n",
        "    \n",
        "    squared_sum = sum((x - y)**2 for x, y in zip(vector1, vector2))\n",
        "    return math.sqrt(squared_sum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V22KPa610LN"
      },
      "source": [
        "### Vectorization(tf-idf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Extracting all the words from vocab.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\freed\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the set of stopwords if not already present\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def filter_unique_english_words(input_file, output_file):\n",
        "    # Regular expression pattern to match English words and numbers\n",
        "    pattern = re.compile(r'[a-zA-Z0-9]+')\n",
        "\n",
        "    # Get the set of English stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        unique_words = set()\n",
        "        for line in infile:\n",
        "            # Find all matches of the pattern in the line\n",
        "            matches = pattern.findall(line)\n",
        "            # Convert each word to lower case and add to unique_words set if it's not a stop word\n",
        "            for word in matches:\n",
        "                word_lower = word.lower()\n",
        "                if word_lower not in stop_words:\n",
        "                    unique_words.add(word_lower)\n",
        "        \n",
        "        # Write unique words to the output file\n",
        "        for word in unique_words:\n",
        "            outfile.write(word + '\\n')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"vocab.txt\"\n",
        "    output_file = \"refactored.txt\"\n",
        "    \n",
        "    filter_unique_english_words(input_file, output_file)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WrIsO2D9cEMB"
      },
      "outputs": [],
      "source": [
        "def Vectorization_tfidf(df_tf):\n",
        "  \"\"\"\n",
        "    Vectorizes the text data in the DataFrame using TF-IDF (Term Frequency-Inverse Document Frequency) representation.\n",
        "\n",
        "    Parameters:\n",
        "        df_tf (tensorflow dataset): The input DataFrame containing the 'Delivery_Desc' column to be vectorized.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A 2D numpy array representing the vectorized text data with TF-IDF scores for each token.\n",
        "\n",
        "    This function preprocesses the text data using TensorFlow's TextVectorization layer with output_mode='tf_idf'.\n",
        "    It creates a vocabulary containing the top 'max_features' most frequent tokens from the input texts, and less frequent tokens are discarded.\n",
        "    Each text sequence is truncated or padded to a maximum length of 'max_len' tokens to ensure uniformity in the input data.\n",
        "    The text data is then vectorized using the TF-IDF method, assigning numerical values to tokens based on their importance\n",
        "    relative to the entire dataset.\n",
        "\n",
        "    \n",
        "    The resulting vectorized data can be used as input for machine learning models in various NLP tasks.\n",
        "  \"\"\"\n",
        "  #The value 1000 here means that the vocabulary will\n",
        "  # contain the top 1000 most frequent tokens from the input texts. Less frequent tokens will be discarded.\n",
        "  max_features=1000\n",
        "  max_len=10\n",
        "  # Create the layer.\n",
        "  vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "  max_tokens=max_features,\n",
        "  output_mode='tf_idf',)\n",
        "  #extracting the delivery desc column\n",
        "  delivery_desc_dataset=df_tf.map(lambda x:x['Delivery_Desc'])\n",
        "  #reshaping the tensors so that it fits in the input layer of the sequential model\n",
        "  reshaped_dataset = delivery_desc_dataset.map(lambda x: tf.expand_dims(x, 0))\n",
        "  #extract all unique words from all the tokens in batches of size 64\n",
        "  vectorize_layer.adapt(delivery_desc_dataset.batch(64))\n",
        "  #viewing all the bag of unique words\n",
        "  vectorize_layer.get_vocabulary()\n",
        "  #creating a model 'Sequential' which sequentially executes steps in layers\n",
        "  model = tf.keras.models.Sequential()\n",
        "  #creating an input layer to input each text\n",
        "  model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "  #creating a layer that assigns numbers to each token based on tf-idf method\n",
        "  model.add(vectorize_layer)\n",
        "  #this is used to make the vectors\n",
        "  vectorized_layer=model.predict(reshaped_dataset)\n",
        "  return vectorized_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def calculate_idf_weights(texts, max_features):\n",
        "    # Create and fit TfidfVectorizer to calculate IDF weights\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features,use_idf=True)\n",
        "    vectorizer.fit(texts)\n",
        "    idf_weights = vectorizer.idf_\n",
        "    idf_weights=np.pad(idf_weights,(14,),'mean')\n",
        "\n",
        "    return idf_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def Vectorization_custom(df_tf,custom_bag_words):\n",
        "  \"\"\"\n",
        "    Vectorizes the text data in the DataFrame using TF-IDF (Term Frequency-Inverse Document Frequency) representation.\n",
        "\n",
        "    Parameters:\n",
        "        df_tf (tensorflow dataset): The input DataFrame containing the 'Delivery_Desc' column to be vectorized.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A 2D numpy array representing the vectorized text data with TF-IDF scores for each token.\n",
        "\n",
        "    This function preprocesses the text data using TensorFlow's TextVectorization layer with output_mode='tf_idf'.\n",
        "    It creates a vocabulary containing the top 'max_features' most frequent tokens from the input texts, and less frequent tokens are discarded.\n",
        "    Each text sequence is truncated or padded to a maximum length of 'max_len' tokens to ensure uniformity in the input data.\n",
        "    The text data is then vectorized using the TF-IDF method, assigning numerical values to tokens based on their importance\n",
        "    relative to the entire dataset.\n",
        "\n",
        "    \n",
        "    The resulting vectorized data can be used as input for machine learning models in various NLP tasks.\n",
        "  \"\"\"\n",
        "  #The value 30438 here means that the vocabulary will\n",
        "  # contain the top 30438 most frequent tokens from the input texts. Less frequent tokens will be discarded.\n",
        "  max_features=30438\n",
        "  # Create the layer.\n",
        "  vectorizer=TfidfVectorizer(max_features=max_features)\n",
        "  vectorizer.fit(custom_bag_words)\n",
        "  idf_weights = calculate_idf_weights(custom_bag_words, max_features)\n",
        "  vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='tf_idf',\n",
        "    vocabulary=custom_bag_words,\n",
        "    idf_weights = idf_weights\n",
        "  )\n",
        "  #extracting the delivery desc column\n",
        "  delivery_desc_dataset=df_tf.map(lambda x:x['Delivery_Desc'])\n",
        "  #reshaping the tensors so that it fits in the input layer of the sequential model\n",
        "  reshaped_dataset = delivery_desc_dataset.map(lambda x: tf.expand_dims(x, 0))\n",
        "  #creating a model 'Sequential' which sequentially executes steps in layers\n",
        "  model = tf.keras.models.Sequential()\n",
        "  #creating an input layer to input each text\n",
        "  model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "  #creating a layer that assigns numbers to each token based on tf-idf method\n",
        "  model.add(vectorize_layer)\n",
        "  #this is used to make the vectors\n",
        "  vectorized_layer=model.predict(reshaped_dataset)\n",
        "  return vectorized_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read().splitlines()\n",
        "\n",
        "texts=read_file('refactored.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QBOkJLgtBr-",
        "outputId": "d3d4fad0-6796-468a-fc52-0e294a886af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "992/992 [==============================] - 2s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# Vectorize the text data using TF-IDF representation\n",
        "vectorized_layer=Vectorization_tfidf(df_tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Vectorize using custom bag of words\n",
        "vectorized_layer_custom=Vectorization_custom(df_tf,texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clustering with vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "#creating a pandas dataframe with 4 columns (cluster_id, customer_id, address, pin code)\n",
        "df = pd.DataFrame(columns=['cluster_id', 'CUSTOMER_ID', 'Delivery_Desc', 'PIN_CODE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data_clustering is a function which performs the clustering algorithm and returns the dataframe containing the n or more sized cluster addresses\n",
        "def clustering_vectors(df_tf,threshold,df):\n",
        "    cluster_id=[]\n",
        "    c_id=1\n",
        "    customer_id = (list(df_tf.take(1))[0])['CUSTOMER_ID'].numpy()\n",
        "    address = (list(df_tf.take(1))[0])['Delivery_Desc'].numpy()\n",
        "    pin_code = (list(df_tf.take(1))[0])['PIN_CODE'].numpy()\n",
        "    cluster_id.append([[c_id,customer_id,address,pin_code]])\n",
        "    #appending this to pandas dataframe df\n",
        "    df.loc[0]=[c_id,str(customer_id),str(address),str(pin_code)]\n",
        "    for i in range(1,len(df_tf)):\n",
        "        customer_id = (list(df_tf.take(i+1))[i])['CUSTOMER_ID'].numpy()\n",
        "        address = (list(df_tf.take(i+1))[i])['Delivery_Desc'].numpy()\n",
        "        pin_code = (list(df_tf.take(i+1))[i])['PIN_CODE'].numpy()\n",
        "        #comparing each address with the last address of the last cluster using cosine similarity\n",
        "        \n",
        "        if (euclidean_distance(vectorized_layer[i],vectorized_layer[i-1])>threshold):\n",
        "            cluster_id[-1].append([[c_id,str(customer_id),str(address),str(pin_code)]])\n",
        "        #if the address is not similar to the last address of the last cluster then it is added to a new cluster\n",
        "        else:\n",
        "            c_id+=1\n",
        "            cluster_id.append([[c_id,str(customer_id),str(address),str(pin_code)]])\n",
        "\n",
        "    #removing all clusters that have length less than 2 clusters\n",
        "    for i in range(len(cluster_id)):\n",
        "        if len(cluster_id[i])<2:\n",
        "            cluster_id[i]=[]\n",
        "    \n",
        "    \n",
        "    return cluster_id\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster=clustering_vectors(df_tf,0.95,df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
